{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/karthi_krish/CSV_files/Machine_learning_course/Position_Salaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:2].values\n",
    "Y = data.iloc[:,2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestRegressor in module sklearn.ensemble.forest:\n",
      "\n",
      "class RandomForestRegressor(ForestRegressor)\n",
      " |  A random forest regressor.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of classifying\n",
      " |  decision trees on various sub-samples of the dataset and uses averaging\n",
      " |  to improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is always the same as the original\n",
      " |  input sample size but the samples are drawn with replacement if\n",
      " |  `bootstrap=True` (default).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : integer, optional (default=10)\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |         The default value of ``n_estimators`` will change from 10 in\n",
      " |         version 0.20 to 100 in version 0.22.\n",
      " |  \n",
      " |  criterion : string, optional (default=\"mse\")\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"mse\" for the mean squared error, which is equal to variance\n",
      " |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      " |      absolute error.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |  max_depth : integer or None, optional (default=None)\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int, float, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int, float, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float, string or None, optional (default=\"auto\")\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, optional (default=0.)\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, (default=1e-7)\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=True)\n",
      " |      Whether bootstrap samples are used when building trees.\n",
      " |  \n",
      " |  oob_score : bool, optional (default=False)\n",
      " |      whether to use out-of-bag samples to estimate\n",
      " |      the R^2 on unseen data.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeRegressor\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  feature_importances_ : array of shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |  \n",
      " |  oob_prediction_ : array of shape = [n_samples]\n",
      " |      Prediction computed with out-of-bag estimate on the training set.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  \n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0,\n",
      " |  ...                              n_estimators=100)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
      " |             max_features='auto', max_leaf_nodes=None,\n",
      " |             min_impurity_decrease=0.0, min_impurity_split=None,\n",
      " |             min_samples_leaf=1, min_samples_split=2,\n",
      " |             min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
      " |             oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      " |  >>> print(regr.feature_importances_)\n",
      " |  [0.18146984 0.81473937 0.00145312 0.00233767]\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-8.32987858]\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  The default value ``max_features=\"auto\"`` uses ``n_features`` \n",
      " |  rather than ``n_features / 3``. The latter was originally suggested in\n",
      " |  [1], whereas the former was more recently justified empirically in [2].\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized \n",
      " |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor, ExtraTreesRegressor\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      ForestRegressor\n",
      " |      abc.NewBase\n",
      " |      BaseForest\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators='warn', criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestRegressor:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      The predicted regression target of an input sample is computed as the\n",
      " |      mean predicted regression targets of the trees in the forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape = [n_samples, n_estimators]\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      " |          Return a node indicator matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |      \n",
      " |      n_nodes_ptr : array of size (n_estimators + 1, )\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |         feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix instead, shape = (n_samples,\n",
      " |          n_samples_fitted], where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([161130.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = RandomForestRegressor(n_estimators = 1000)\n",
    "regressor = regressor.fit(X,Y)\n",
    "regressor.predict([[6.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,Y,color = 'blue')\n",
    "X_grid = np.arange(min(X),max(X),0.01)\n",
    "X_grid = X_grid.reshape(len(X_grid),1)\n",
    "plt.plot(X_grid,regressor.predict(X_grid),color = 'red')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Salaries')\n",
    "plt.title('Truth or Bluff (Random Forest Regression)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
